experiment: celeb
n_iter: 10
seed: 0

sweep:
  source:
  - data::seed:                        [0]
    data::innout::dataset::args::seed: [0]
    source::model::seed:               [0]
  - data::seed:                        [1]
    data::innout::dataset::args::seed: [1]
    source::model::seed:               [1]
  - data::seed:                        [2]
    data::innout::dataset::args::seed: [2]
    source::model::seed:               [2]
  - data::seed:                        [3]
    data::innout::dataset::args::seed: [3]
    source::model::seed:               [3]
  - data::seed:                        [4]
    data::innout::dataset::args::seed: [4]
    source::model::seed:               [4]
  target:
  - data::seed:                        [0]
    data::innout::dataset::args::seed: [0]
    target::model::seed:               [0]
  - data::seed:                        [1]
    data::innout::dataset::args::seed: [1]
    target::model::seed:               [1]
  - data::seed:                        [2]
    data::innout::dataset::args::seed: [2]
    target::model::seed:               [2]
  - data::seed:                        [3]
    data::innout::dataset::args::seed: [3]
    target::model::seed:               [3]
  - data::seed:                        [4]
    data::innout::dataset::args::seed: [4]
    target::model::seed:               [4]
  dalupi:
  - x2w::use_source_pi:                [true, false]
    data::seed:                        [0]
    data::innout::dataset::args::seed: [0]
    x2w::model::seed:                  [0]
    w2y::model::seed:                  [0]
  - x2w::use_source_pi:                [true, false]
    data::seed:                        [1]
    data::innout::dataset::args::seed: [1]
    x2w::model::seed:                  [1]
    w2y::model::seed:                  [1]
  - x2w::use_source_pi:                [true, false]
    data::seed:                        [2]
    data::innout::dataset::args::seed: [2]
    x2w::model::seed:                  [2]
    w2y::model::seed:                  [2]
  - x2w::use_source_pi:                [true, false]
    data::seed:                        [3]
    data::innout::dataset::args::seed: [3]
    x2w::model::seed:                  [3]
    w2y::model::seed:                  [3]
  - x2w::use_source_pi:                [true, false]
    data::seed:                        [4]
    data::innout::dataset::args::seed: [4]
    x2w::model::seed:                  [4]
    w2y::model::seed:                  [4]
  adapt_dann:
  - data::seed:                        [0]
    data::innout::dataset::args::seed: [0]
    adapt_dann::model::random_state:   [0]
  - data::seed:                        [1]
    data::innout::dataset::args::seed: [1]
    adapt_dann::model::random_state:   [1]
  - data::seed:                        [2]
    data::innout::dataset::args::seed: [2]
    adapt_dann::model::random_state:   [2]
  - data::seed:                        [3]
    data::innout::dataset::args::seed: [3]
    adapt_dann::model::random_state:   [3]
  - data::seed:                        [4]
    data::innout::dataset::args::seed: [4]
    adapt_dann::model::random_state:   [4]
  adapt_mdd:
  - data::seed:                        [0]
    data::innout::dataset::args::seed: [0]
    adapt_mdd::model::random_state:    [0]
  - data::seed:                        [1]
    data::innout::dataset::args::seed: [1]
    adapt_mdd::model::random_state:    [1]
  - data::seed:                        [2]
    data::innout::dataset::args::seed: [2]
    adapt_mdd::model::random_state:    [2]
  - data::seed:                        [3]
    data::innout::dataset::args::seed: [3]
    adapt_mdd::model::random_state:    [3]
  - data::seed:                        [4]
    data::innout::dataset::args::seed: [4]
    adapt_mdd::model::random_state:    [4]

source: &source
- default::batch_size: [16, 32, 64]
  default::lr: [1.0e-4, 1.0e-3]
  default::optimizer__weight_decay: [1.0e-4, 1.0e-3]
  default::module::featurizer__p_dropout: [0, 0.1, 0.2, 0.5]
  default::module::task__is_nonlinear: [true, false]
  callbacks__lr_scheduler__step_size: [15, 30, 100]

target: *source

dalupi:
- default::batch_size: [16, 32, 64]
  default::lr: [1.0e-5, 1.0e-4, 1.0e-3]
  default::optimizer__weight_decay: [1.0e-4, 1.0e-3]
  callbacks__lr_scheduler__step_size: [15, 30, 100]

adapt_dann:
# learning_rate = 1.0e-4, weight_decay = 1.0e-4
## constant learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
# learning_rate = 1.0e-4, weight_decay = 1.0e-3
## constant learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
# learning_rate = 1.0e-3, weight_decay = 1.0e-4
## constant learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
# learning_rate = 1.0e-3, weight_decay = 1.0e-3
## constant learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]

adapt_mdd:
# learning_rate = 1.0e-4, weight_decay = 1.0e-4
## constant learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
# learning_rate = 1.0e-4, weight_decay = 1.0e-3
## constant learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
# learning_rate = 1.0e-3, weight_decay = 1.0e-4
## constant learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
# learning_rate = 1.0e-3, weight_decay = 1.0e-3
## constant learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
