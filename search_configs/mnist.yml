experiment: mnist
n_iter: 10
seed: 0

sweep:
- data::skew:                      [0.2, 0.4, 0.6, 0.8, 1.0]
  data::seed:                      [0]
  source::model::seed:             [0]
  target::model::seed:             [0]
  x2w::model::seed:                [0]
  w2y::model::seed:                [0]
  dann::model::seed:               [0]
  adapt_dann::model::random_state: [0]
  adapt_mdd::model::random_state:  [0]
- data::skew:                      [0.2, 0.4, 0.6, 0.8, 1.0]
  data::seed:                      [1]
  source::model::seed:             [1]
  target::model::seed:             [1]
  x2w::model::seed:                [1]
  w2y::model::seed:                [1]
  dann::model::seed:               [1]
  adapt_dann::model::random_state: [1]
  adapt_mdd::model::random_state:  [1]
- data::skew:                      [0.2, 0.4, 0.6, 0.8, 1.0]
  data::seed:                      [2]
  source::model::seed:             [2]
  target::model::seed:             [2]
  x2w::model::seed:                [2]
  w2y::model::seed:                [2]
  dann::model::seed:               [2]
  adapt_dann::model::random_state: [2]
  adapt_mdd::model::random_state:  [2]
- data::skew:                      [0.2, 0.4, 0.6, 0.8, 1.0]
  data::seed:                      [3]
  source::model::seed:             [3]
  target::model::seed:             [3]
  x2w::model::seed:                [3]
  w2y::model::seed:                [3]
  dann::model::seed:               [3]
  adapt_dann::model::random_state: [3]
  adapt_mdd::model::random_state:  [3]
- data::skew:                      [0.2, 0.4, 0.6, 0.8, 1.0]
  data::seed:                      [4]
  source::model::seed:             [4]
  target::model::seed:             [4]
  x2w::model::seed:                [4]
  w2y::model::seed:                [4]
  dann::model::seed:               [4]
  adapt_dann::model::random_state: [4]
  adapt_mdd::model::random_state:  [4]
- data::skew:                      [0.2, 0.4, 0.6, 0.8, 1.0]
  data::seed:                      [5]
  source::model::seed:             [5]
  target::model::seed:             [5]
  x2w::model::seed:                [5]
  w2y::model::seed:                [5]
  dann::model::seed:               [5]
  adapt_dann::model::random_state: [5]
  adapt_mdd::model::random_state:  [5]
- data::skew:                      [0.2, 0.4, 0.6, 0.8, 1.0]
  data::seed:                      [6]
  source::model::seed:             [6]
  target::model::seed:             [6]
  x2w::model::seed:                [6]
  w2y::model::seed:                [6]
  dann::model::seed:               [6]
  adapt_dann::model::random_state: [6]
  adapt_mdd::model::random_state:  [6]
- data::skew:                      [0.2, 0.4, 0.6, 0.8, 1.0]
  data::seed:                      [7]
  source::model::seed:             [7]
  target::model::seed:             [7]
  x2w::model::seed:                [7]
  w2y::model::seed:                [7]
  dann::model::seed:               [7]
  adapt_dann::model::random_state: [7]
  adapt_mdd::model::random_state:  [7]
- data::skew:                      [0.2, 0.4, 0.6, 0.8, 1.0]
  data::seed:                      [8]
  source::model::seed:             [8]
  target::model::seed:             [8]
  x2w::model::seed:                [8]
  w2y::model::seed:                [8]
  dann::model::seed:               [8]
  adapt_dann::model::random_state: [8]
  adapt_mdd::model::random_state:  [8]
- data::skew:                      [0.2, 0.4, 0.6, 0.8, 1.0]
  data::seed:                      [9]
  source::model::seed:             [9]
  target::model::seed:             [9]
  x2w::model::seed:                [9]
  w2y::model::seed:                [9]
  dann::model::seed:               [9]
  adapt_dann::model::random_state: [9]
  adapt_mdd::model::random_state:  [9]

source: &source
- default::batch_size: [16, 32, 64]
  default::lr: [1.0e-4, 1.0e-3]
  default::optimizer__weight_decay: [1.0e-4, 1.0e-3]
  default::module::featurizer__p_dropout: [0, 0.1, 0.2, 0.5]
  default::module::task__is_nonlinear: [true, false]
  callbacks__lr_scheduler__step_size: [15, 30, 100]

target: *source

dalupi:
- default::batch_size: [16, 32, 64]
  default::lr: [1.0e-4, 1.0e-3]
  default::optimizer__weight_decay: [1.0e-4, 1.0e-3]
  callbacks__lr_scheduler__step_size: [15, 30, 100]

dann:
# weight_decay = 1.0e-4
- default::batch_size: [16, 32, 64]
  default::lr: [1.0e-4, 1.0e-3]
  dann::model::d_steps_per_g_step: [1, 2]
  dann::model::grad_penalty: [0, 0.01, 0.1]
  dann::model::optimizer_generator__weight_decay: [1.0e-4]
  dann::model::optimizer_discriminator__weight_decay: [1.0e-4]
  #dann::module::featurizer__n_trainable_layers: [1, 2, 3, 4, 5]
  dann::module::featurizer__p_dropout: [0, 0.1, 0.2, 0.5]
  dann::module::discriminator__width: [64, 128, 256]
  dann::module::discriminator__depth: [2, 3]
  dann::module::task__is_nonlinear: [true, false]
  callbacks__lr_scheduler__step_size: [15, 30, 100]
# weight_decay = 1.0e-3
- default::batch_size: [16, 32, 64]
  default::lr: [1.0e-4, 1.0e-3]
  dann::model::d_steps_per_g_step: [1, 2]
  dann::model::grad_penalty: [0, 0.01, 0.1]
  dann::model::optimizer_generator__weight_decay: [1.0e-3]
  dann::model::optimizer_discriminator__weight_decay: [1.0e-3]
  #dann::module::featurizer__n_trainable_layers: [1, 2, 3, 4, 5]
  dann::module::featurizer__p_dropout: [0, 0.1, 0.2, 0.5]
  dann::module::discriminator__width: [64, 128, 256]
  dann::module::discriminator__depth: [2, 3]
  dann::module::task__is_nonlinear: [true, false]
  callbacks__lr_scheduler__step_size: [15, 30, 100]

adapt_dann:
# learning_rate = 1.0e-4, weight_decay = 1.0e-4
## constant learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
# learning_rate = 1.0e-4, weight_decay = 1.0e-3
## constant learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
# learning_rate = 1.0e-3, weight_decay = 1.0e-4
## constant learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
# learning_rate = 1.0e-3, weight_decay = 1.0e-3
## constant learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]

adapt_mdd:
# learning_rate = 1.0e-4, weight_decay = 1.0e-4
## constant learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
# learning_rate = 1.0e-4, weight_decay = 1.0e-3
## constant learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
# learning_rate = 1.0e-3, weight_decay = 1.0e-4
## constant learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
# learning_rate = 1.0e-3, weight_decay = 1.0e-3
## constant learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
