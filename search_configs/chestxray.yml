experiment: chestxray
n_iter: 10
seed: 0

sweep:
- data::pipeline::num_train_sample: [457, 1457, 2457, 5457, 10457, 20457, 30457]
  data::seed:                       [0]
  data::pipeline::seed:             [0]
  source::model::seed:              [0]
  target::model::seed:              [0]
  dalupi::model::seed:              [0]
  dann::model::seed:                [0]
  adapt_dann::model::random_state:  [0]
  adapt_mdd::model::random_state:   [0]
- data::pipeline::num_train_sample: [457, 1457, 2457, 5457, 10457, 20457, 30457]
  data::seed:                       [1]
  data::pipeline::seed:             [1]
  source::model::seed:              [1]
  target::model::seed:              [1]
  dalupi::model::seed:              [1]
  dann::model::seed:                [1]
  adapt_dann::model::random_state:  [1]
  adapt_mdd::model::random_state:   [1]
- data::pipeline::num_train_sample: [457, 1457, 2457, 5457, 10457, 20457, 30457]
  data::seed:                       [2]
  data::pipeline::seed:             [2]
  source::model::seed:              [2]
  target::model::seed:              [2]
  dalupi::model::seed:              [2]
  dann::model::seed:                [2]
  adapt_dann::model::random_state:  [2]
  adapt_mdd::model::random_state:   [2]
- data::pipeline::num_train_sample: [457, 1457, 2457, 5457, 10457, 20457, 30457]
  data::seed:                       [3]
  data::pipeline::seed:             [3]
  source::model::seed:              [3]
  target::model::seed:              [3]
  dalupi::model::seed:              [3]
  dann::model::seed:                [3]
  adapt_dann::model::random_state:  [3]
  adapt_mdd::model::random_state:   [3]
- data::pipeline::num_train_sample: [457, 1457, 2457, 5457, 10457, 20457, 30457]
  data::seed:                       [4]
  data::pipeline::seed:             [4]
  source::model::seed:              [4]
  target::model::seed:              [4]
  dalupi::model::seed:              [4]
  dann::model::seed:                [4]
  adapt_dann::model::random_state:  [4]
  adapt_mdd::model::random_state:   [4]

source: &source
- default::batch_size: [16, 32, 64]
  default::lr: [1.0e-4, 1.0e-3]
  default::optimizer__weight_decay: [1.0e-4, 1.0e-3]
  default::module::featurizer__p_dropout: [0, 0.1, 0.2, 0.5]
  default::module::task__is_nonlinear: [true, false]
  finetuning::n_layers: [3, 4, 5]
  finetuning::lr: [1.0e-5, 1.0e-4]

target: *source

dalupi:
- default::batch_size: [16, 32, 64]
  default::lr: [1.0e-4]  # 1.0e-3 is too large
  default::optimizer__weight_decay: [1.0e-4, 1.0e-3]
  dalupi::module::rpn_fg_iou_thresh: [0.6, 0.7, 0.8, 0.9]
  dalupi::module::rpn_bg_iou_thresh: [0.2, 0.3, 0.4]
  dalupi::module::rpn_batch_size_per_image: [32, 64, 128, 256]
  dalupi::module::rpn_positive_fraction: [0.4, 0.5, 0.6, 0.7]
  dalupi::module::rpn_nms_thresh: [0.6, 0.7, 0.8]
  dalupi::module::box_roi_pool_output_size: [5, 7, 9]
  dalupi::module::box_fg_iou_thresh: [0.5, 0.6]
  dalupi::module::box_bg_iou_thresh: [0.4, 0.5]
  dalupi::module::box_batch_size_per_image: [16, 32, 64, 128]
  dalupi::module::box_positive_fraction: [0.2, 0.25, 0.3]
  dalupi::module::box_nms_thresh: [0.4, 0.5, 0.6]
  dalupi::module::box_detections_per_img: [25, 50, 75, 100]
  finetuning::n_layers: [3, 4, 5]
  finetuning::lr: [1.0e-5, 1.0e-4]

dann:
# weight_decay = 1.0e-4
- default::batch_size: [16, 32, 64]
  default::lr: [1.0e-4, 1.0e-3]
  dann::model::d_steps_per_g_step: [1, 2]
  dann::model::grad_penalty: [0, 0.01, 0.1]
  dann::model::optimizer_generator__weight_decay: [1.0e-4]
  dann::model::optimizer_discriminator__weight_decay: [1.0e-4]
  dann::module::featurizer__n_trainable_layers: [1, 2, 3, 4, 5]
  dann::module::featurizer__p_dropout: [0, 0.1, 0.2, 0.5]
  dann::module::discriminator__width: [64, 128, 256]
  dann::module::discriminator__depth: [2, 3]
  dann::module::task__is_nonlinear: [true, false]
  callbacks__lr_scheduler__step_size: [15, 30, 100]
# weight_decay = 1.0e-3
- default::batch_size: [16, 32, 64]
  default::lr: [1.0e-4, 1.0e-3]
  dann::model::d_steps_per_g_step: [1, 2]
  dann::model::grad_penalty: [0, 0.01, 0.1]
  dann::model::optimizer_generator__weight_decay: [1.0e-3]
  dann::model::optimizer_discriminator__weight_decay: [1.0e-3]
  dann::module::featurizer__n_trainable_layers: [1, 2, 3, 4, 5]
  dann::module::featurizer__p_dropout: [0, 0.1, 0.2, 0.5]
  dann::module::discriminator__width: [64, 128, 256]
  dann::module::discriminator__depth: [2, 3]
  dann::module::task__is_nonlinear: [true, false]
  callbacks__lr_scheduler__step_size: [15, 30, 100]

adapt_dann:
# learning_rate = 1.0e-4, weight_decay = 1.0e-4
## constant learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_dann::model::encoder::n_trainable_layers: [1, 2, 3, 4, 5]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_dann::model::encoder::n_trainable_layers: [1, 2, 3, 4, 5]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
# learning_rate = 1.0e-4, weight_decay = 1.0e-3
## constant learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_dann::model::encoder::n_trainable_layers: [1, 2, 3, 4, 5]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_dann::model::encoder::n_trainable_layers: [1, 2, 3, 4, 5]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
# learning_rate = 1.0e-3, weight_decay = 1.0e-4
## constant learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_dann::model::encoder::n_trainable_layers: [1, 2, 3, 4, 5]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_dann::model::encoder::n_trainable_layers: [1, 2, 3, 4, 5]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
# learning_rate = 1.0e-3, weight_decay = 1.0e-3
## constant learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_dann::model::encoder::n_trainable_layers: [1, 2, 3, 4, 5]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_dann::model::batch_size: [16, 32, 64]
  adapt_dann::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_dann::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_dann::model::optimizer::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_dann::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_dann::model::encoder::n_trainable_layers: [1, 2, 3, 4, 5]
  adapt_dann::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_dann::model::discriminator::width: [64, 128, 256]
  adapt_dann::model::discriminator::depth: [2, 3]
  adapt_dann::model::task::is_nonlinear: [true, false]
  adapt_dann::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]

adapt_mdd:
# learning_rate = 1.0e-4, weight_decay = 1.0e-4
## constant learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_mdd::model::encoder::n_trainable_layers: [1, 2, 3, 4, 5]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_mdd::model::encoder::n_trainable_layers: [1, 2, 3, 4, 5]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
# learning_rate = 1.0e-4, weight_decay = 1.0e-3
## constant learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_mdd::model::encoder::n_trainable_layers: [1, 2, 3, 4, 5]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-4]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_mdd::model::encoder::n_trainable_layers: [1, 2, 3, 4, 5]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
# learning_rate = 1.0e-3, weight_decay = 1.0e-4
## constant learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_mdd::model::encoder::n_trainable_layers: [1, 2, 3, 4, 5]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-4]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-4]
  adapt_mdd::model::encoder::n_trainable_layers: [1, 2, 3, 4, 5]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
# learning_rate = 1.0e-3, weight_decay = 1.0e-3
## constant learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_mdd::model::encoder::n_trainable_layers: [1, 2, 3, 4, 5]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
## decaying learning rate
- adapt_mdd::model::batch_size: [16, 32, 64]
  adapt_mdd::model::optimizer::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_enc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_enc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer_disc::learning_rate::mu_0: [1.0e-3]
  adapt_mdd::model::optimizer_disc::learning_rate::alpha: [1.0]
  adapt_mdd::model::optimizer::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_enc::weight_decay: [1.0e-3]
  adapt_mdd::model::optimizer_disc::weight_decay: [1.0e-3]
  adapt_mdd::model::encoder::n_trainable_layers: [1, 2, 3, 4, 5]
  adapt_mdd::model::encoder::dropout: [0, 0.1, 0.2, 0.5]
  adapt_mdd::model::task::is_nonlinear: [true, false]
  adapt_mdd::model::task::max_norm: [0.5, 1.0, 2.0]
  adapt_mdd::model::callbacks::update_lambda::gamma: [0.1, 1.0, 10.0]
